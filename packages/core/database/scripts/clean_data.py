#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –æ –º–æ–¥–µ–ª—è—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π.

–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç raw_vehicle_data.csv –∏ —Å–æ–∑–¥–∞–µ—Ç cleaned_vehicle_data.csv
—Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –±—Ä–µ–Ω–¥–∞–º–∏, –≥–æ–¥–∞–º–∏ –∏ —Ç–∏–ø–∞–º–∏ –∫—É–∑–æ–≤–∞.
"""

import sys
import re
from pathlib import Path
from typing import Tuple, Optional
import pandas as pd


# –î–æ–±–∞–≤–ª—è–µ–º parent directory –≤ Python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


# –ú–∞–ø–ø–∏–Ω–≥ —Ä—É—Å—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –±—Ä–µ–Ω–¥–æ–≤ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ
BRAND_MAPPING = {
    # –†—É—Å—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    "–¢–û–ô–û–¢–ê": "Toyota",
    "–•–û–ù–î–ê": "Honda",
    "–ú–ê–ó–î–ê": "Mazda",
    "–ù–ò–°–°–ê–ù": "Nissan",
    "–ú–ò–¢–°–£–ë–ò–°–ò": "Mitsubishi",
    "–ú–ò–¢–°–£–ë–ò–®–ò": "Mitsubishi",
    "–°–£–ë–ê–†–£": "Subaru",
    "–°–£–ó–£–ö–ò": "Suzuki",
    "–õ–ï–ö–°–£–°": "Lexus",
    "–ò–ù–§–ò–ù–ò–¢–ò": "Infiniti",
    "–ë–ú–í": "BMW",
    "–ú–ï–†–°–ï–î–ï–°": "Mercedes-Benz",
    "–ú–ï–†–°–ï–î–ï–°-–ë–ï–ù–¶": "Mercedes-Benz",
    "–ê–£–î–ò": "Audi",
    "–§–û–õ–¨–ö–°–í–ê–ì–ï–ù": "Volkswagen",
    "–§–û–õ–¨–¶–í–ê–ì–ï–ù": "Volkswagen",
    "–ü–û–†–®–ï": "Porsche",
    "–†–ï–ù–û": "Renault",
    "–ü–ï–ñ–û": "Peugeot",
    "–°–ò–¢–†–û–ï–ù": "Citroen",
    "–§–ò–ê–¢": "Fiat",
    "–§–û–†–î": "Ford",
    "–®–ï–í–†–û–õ–ï": "Chevrolet",
    "–î–û–î–ñ": "Dodge",
    "–î–ñ–ò–ü": "Jeep",
    "–ö–†–ê–ô–°–õ–ï–†": "Chrysler",
    "–ö–ê–î–ò–õ–õ–ê–ö": "Cadillac",
    "–•–ï–ù–î–ê–ô": "Hyundai",
    "–•–ï–ù–î–≠": "Hyundai",
    "–ö–ò–ê": "Kia",
    "–î–≠–í–û": "Daewoo",
    "–î–≠–í–û–û": "Daewoo",
    "–°–°–ê–ù–ì –ô–û–ù–ì": "SsangYong",
    "–°–°–ê–ù–™–Å–ù": "SsangYong",
    "–ß–ï–†–ò": "Chery",
    "–î–ñ–ò–õ–ò": "Geely",
    "–õ–ê–î–ê": "Lada",
    "–ì–ê–ó": "GAZ",
    "–£–ê–ó": "UAZ",

    # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–≥–∏—Å—Ç—Ä–∞)
    "TOYOTA": "Toyota",
    "HONDA": "Honda",
    "MAZDA": "Mazda",
    "NISSAN": "Nissan",
    "MITSUBISHI": "Mitsubishi",
    "SUBARU": "Subaru",
    "SUZUKI": "Suzuki",
    "LEXUS": "Lexus",
    "INFINITI": "Infiniti",
    "BMW": "BMW",
    "MERCEDES": "Mercedes-Benz",
    "MERCEDES-BENZ": "Mercedes-Benz",
    "AUDI": "Audi",
    "VOLKSWAGEN": "Volkswagen",
    "VW": "Volkswagen",
    "PORSCHE": "Porsche",
    "RENAULT": "Renault",
    "PEUGEOT": "Peugeot",
    "CITROEN": "Citroen",
    "FIAT": "Fiat",
    "FORD": "Ford",
    "CHEVROLET": "Chevrolet",
    "DODGE": "Dodge",
    "JEEP": "Jeep",
    "CHRYSLER": "Chrysler",
    "CADILLAC": "Cadillac",
    "HYUNDAI": "Hyundai",
    "KIA": "Kia",
    "DAEWOO": "Daewoo",
    "SSANGYONG": "SsangYong",
    "CHERY": "Chery",
    "GEELY": "Geely",
    "LADA": "Lada",
    "GAZ": "GAZ",
    "UAZ": "UAZ",
    "VOLVO": "Volvo",
    "SAAB": "Saab",
    "SKODA": "Skoda",
    "SEAT": "Seat",
    "OPEL": "Opel",
    "LAND ROVER": "Land Rover",
    "RANGE ROVER": "Land Rover",
    "JAGUAR": "Jaguar",
}


# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –∫—É–∑–æ–≤–∞
BODY_TYPE_KEYWORDS = {
    "sedan": ["sedan", "—Å–µ–¥–∞–Ω"],
    "suv": ["suv", "–≤–Ω–µ–¥–æ—Ä–æ–∂–Ω–∏–∫", "–∫—Ä–æ—Å—Å–æ–≤–µ—Ä", "crossover"],
    "minivan": ["minivan", "–º–∏–Ω–∏–≤—ç–Ω", "–º–∏–Ω–∏–≤–µ–Ω", "van"],
    "hatchback": ["hatchback", "—Ö—ç—Ç—á–±–µ–∫", "—Ö–µ—Ç—á–±—ç–∫", "—Ö—ç—Ç—á"],
    "wagon": ["wagon", "—É–Ω–∏–≤–µ—Ä—Å–∞–ª", "estate"],
    "coupe": ["coupe", "–∫—É–ø–µ"],
    "convertible": ["convertible", "–∫–∞–±—Ä–∏–æ–ª–µ—Ç", "cabriolet"],
    "pickup": ["pickup", "–ø–∏–∫–∞–ø"],
}


def normalize_brand(brand: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞.

    Args:
        brand: –ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞

    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
    """
    if not brand:
        return ""

    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è –ø–æ–∏—Å–∫–∞
    brand_upper = brand.strip().upper()

    # –ò—â–µ–º –≤ –º–∞–ø–ø–∏–Ω–≥–µ
    normalized = BRAND_MAPPING.get(brand_upper)

    if normalized:
        return normalized

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
    return brand.strip().title()


def parse_year_range(year_str: str) -> Tuple[Optional[int], Optional[int]]:
    """
    –ü–∞—Ä—Å–∏—Ç –≥–æ–¥ –∏–ª–∏ –¥–∏–∞–ø–∞–∑–æ–Ω –ª–µ—Ç.

    –ü—Ä–∏–º–µ—Ä—ã:
        "2018" -> (2018, 2018)
        "2018-2022" -> (2018, 2022)
        "2018-–Ω.–≤." -> (2018, None)
        "–¥–æ 2006–≥" -> (None, 2006)
        "" -> (None, None)

    Args:
        year_str: –°—Ç—Ä–æ–∫–∞ —Å –≥–æ–¥–æ–º/–¥–∏–∞–ø–∞–∑–æ–Ω–æ–º

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (–≥–æ–¥_—Å, –≥–æ–¥_–ø–æ)
    """
    if not year_str or year_str.strip() == '':
        return (None, None)

    year_str = year_str.strip().lower()

    # "–¥–æ 2006–≥" -> (None, 2006)
    match = re.search(r'–¥–æ\s*(\d{4})', year_str)
    if match:
        return (None, int(match.group(1)))

    # "—Å 2018" -> (2018, None)
    match = re.search(r'—Å\s*(\d{4})', year_str)
    if match:
        return (int(match.group(1)), None)

    # "2018-–Ω.–≤." –∏–ª–∏ "2018-" -> (2018, None)
    match = re.search(r'(\d{4})\s*[-‚Äì]\s*(?:–Ω\.?\s*–≤\.?|$)', year_str)
    if match:
        return (int(match.group(1)), None)

    # "2018-2022" -> (2018, 2022)
    match = re.search(r'(\d{4})\s*[-‚Äì]\s*(\d{4})', year_str)
    if match:
        year_from = int(match.group(1))
        year_to = int(match.group(2))
        return (year_from, year_to)

    # –ü—Ä–æ—Å—Ç–æ –≥–æ–¥ "2018" -> (2018, 2018)
    match = re.search(r'(\d{4})', year_str)
    if match:
        year = int(match.group(1))
        return (year, year)

    return (None, None)


def detect_body_type(model: str, description: str) -> Optional[str]:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫—É–∑–æ–≤–∞ –∏–∑ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è.

    Args:
        model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        description: –û–ø–∏—Å–∞–Ω–∏–µ

    Returns:
        –¢–∏–ø –∫—É–∑–æ–≤–∞ –∏–ª–∏ None
    """
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è –ø–æ–∏—Å–∫–∞
    text = f"{model} {description}".lower()

    for body_type, keywords in BODY_TYPE_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text:
                return body_type

    return None


def clean_model_name(model: str) -> str:
    """
    –û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤.

    Args:
        model: –ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏

    Returns:
        –û—á–∏—â–µ–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
    """
    if not model:
        return ""

    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    model = re.sub(r'\s+', ' ', model.strip())

    # –£–±–∏—Ä–∞–µ–º —Ç–æ—á–∫–∏ –≤ –∫–æ–Ω—Ü–µ
    model = model.rstrip('.')

    return model


def clean_data(input_path: str, output_path: str) -> pd.DataFrame:
    """
    –û—á–∏—â–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ.

    Args:
        input_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É CSV
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ CSV

    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π DataFrame
    """
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑: {input_path}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_csv(input_path, encoding='utf-8-sig')

    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –±—Ä–µ–Ω–¥—ã
    print("\nüîß –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±—Ä–µ–Ω–¥–æ–≤...")
    df['brand_normalized'] = df['brand'].apply(normalize_brand)

    brands_before = df['brand'].nunique()
    brands_after = df['brand_normalized'].nunique()
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤: {brands_before} ‚Üí {brands_after}")

    # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
    print("\nüîß –û—á–∏—Å—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π...")
    df['model_cleaned'] = df['model'].apply(clean_model_name)

    # –ü–∞—Ä—Å–∏–º –≥–æ–¥—ã
    print("\nüîß –ü–∞—Ä—Å–∏–Ω–≥ –≥–æ–¥–æ–≤...")
    year_parsed = df['year'].apply(parse_year_range)
    df['year_from'] = year_parsed.apply(lambda x: x[0])
    df['year_to'] = year_parsed.apply(lambda x: x[1])

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥–æ–¥–∞–º
    with_years = df[(df['year_from'].notna()) | (df['year_to'].notna())]
    print(f"   –ó–∞–ø–∏—Å–µ–π —Å –≥–æ–¥–∞–º–∏: {len(with_years)} –∏–∑ {len(df)}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –∫—É–∑–æ–≤–∞
    print("\nüîß –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∫—É–∑–æ–≤–∞...")
    df['body_type'] = df.apply(
        lambda row: detect_body_type(
            row['model_cleaned'],
            row['description'] if pd.notna(row['description']) else ''
        ),
        axis=1
    )

    with_body_type = df[df['body_type'].notna()]
    print(f"   –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ç–∏–ø –∫—É–∑–æ–≤–∞: {len(with_body_type)} –∏–∑ {len(df)}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∫—É–∑–æ–≤–∞
    if len(with_body_type) > 0:
        print("\n   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∫—É–∑–æ–≤–∞:")
        for body_type, count in df['body_type'].value_counts().items():
            print(f"     {body_type}: {count}")

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    print("\nüîß –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
    before_dedup = len(df)
    df = df.drop_duplicates(
        subset=['brand_normalized', 'model_cleaned', 'year_from', 'year_to'],
        keep='first'
    )
    after_dedup = len(df)
    removed = before_dedup - after_dedup

    if removed > 0:
        print(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed}")

    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
    df_final = df[[
        'id',
        'brand_normalized',
        'model_cleaned',
        'year_from',
        'year_to',
        'body_type',
        'description'
    ]].rename(columns={
        'brand_normalized': 'brand',
        'model_cleaned': 'model'
    })

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {output_path}")
    df_final.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(df_final)}")
    print(f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {Path(output_path).stat().st_size / 1024:.1f} KB")

    return df_final


def print_statistics(df: pd.DataFrame) -> None:
    """
    –í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ—á–∏—â–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º.

    Args:
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
    """
    print("\n" + "=" * 60)
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ß–ò–©–ï–ù–ù–´–• –î–ê–ù–ù–´–•")
    print("=" * 60)

    print(f"\nüìù –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
    print(f"üè¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤: {df['brand'].nunique()}")
    print(f"üöó –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {df['model'].nunique()}")

    # –¢–æ–ø-10 –±—Ä–µ–Ω–¥–æ–≤
    print("\nüèÜ –¢–æ–ø-10 –±—Ä–µ–Ω–¥–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –º–æ–¥–µ–ª–µ–π:")
    for i, (brand, count) in enumerate(df['brand'].value_counts().head(10).items(), 1):
        print(f"   {i:2}. {brand:20} {count:4} –º–æ–¥–µ–ª–µ–π")

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –∫—É–∑–æ–≤–∞
    print("\nüöô –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –∫—É–∑–æ–≤–∞:")
    body_type_counts = df['body_type'].value_counts()
    for body_type, count in body_type_counts.items():
        percentage = count / len(df) * 100
        print(f"   {body_type or 'unknown':15} {count:4} ({percentage:5.1f}%)")

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º
    print("\nüìÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º:")
    with_years = df[(df['year_from'].notna()) | (df['year_to'].notna())]
    without_years = len(df) - len(with_years)
    print(f"   –° —É–∫–∞–∑–∞–Ω–∏–µ–º –≥–æ–¥–∞:  {len(with_years):4} ({len(with_years)/len(df)*100:5.1f}%)")
    print(f"   –ë–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –≥–æ–¥–∞: {without_years:4} ({without_years/len(df)*100:5.1f}%)")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""

    print("=" * 60)
    print("üßπ –û–ß–ò–°–¢–ö–ê –ò –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• –û –ú–û–î–ï–õ–Ø–•")
    print("=" * 60)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏
    project_root = Path(__file__).parent.parent.parent
    input_path = project_root / "database" / "data" / "raw_vehicle_data.csv"
    output_path = project_root / "database" / "data" / "cleaned_vehicle_data.csv"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    if not input_path.exists():
        print(f"\n‚ùå –û—à–∏–±–∫–∞: –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        print(f"   –û–∂–∏–¥–∞–µ–º—ã–π –ø—É—Ç—å: {input_path}")
        print(f"\n–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/extract_pdf_data.py")
        sys.exit(1)

    try:
        # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = clean_data(str(input_path), str(output_path))

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print_statistics(df)

        print("\n" + "=" * 60)
        print("‚úÖ –û–ß–ò–°–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û")
        print("=" * 60)
        print(f"\n–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: python scripts/import_to_database.py")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
